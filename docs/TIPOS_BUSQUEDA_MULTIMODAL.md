# ğŸ” Tipos de BÃºsqueda Multimodal en el Sistema RAG

## Resumen de Casos de Prueba Implementados

Este documento explica los **5 tipos de bÃºsqueda** que implementa nuestro sistema RAG multimodal, cubriendo todas las combinaciones posibles de entrada/salida.

---

## ğŸ“Š Matriz de Tipos de BÃºsqueda

| Caso | Entrada       | Proceso                      | Salida              | Endpoint         | Multimodal? |
| ---- | ------------- | ---------------------------- | ------------------- | ---------------- | ----------- |
| 1    | Texto         | Vector Search + LLM          | Texto + ImÃ¡genes    | POST /query      | âœ… SÃ­       |
| 2    | Texto + Filtros | Vector Search + Filtros + LLM | Texto + ImÃ¡genes    | POST /query      | âœ… SÃ­       |
| 3    | Imagen        | Vector Search (embedding)    | ImÃ¡genes similares  | GET /similar/:id | âœ… SÃ­       |
| 4    | Texto         | Vector Search + LLM complejo | Texto enriquecido   | POST /query      | âœ… SÃ­       |
| 5    | Texto         | Vector Search (sin LLM)      | Solo imÃ¡genes       | GET /search      | âœ… SÃ­       |

---

## ğŸ¯ Caso 1: BÃºsqueda SemÃ¡ntica (TEXTO â†’ TEXTO + IMÃGENES)

### DescripciÃ³n

RAG completo: el usuario hace una pregunta en lenguaje natural y recibe una respuesta generada por el LLM + las imÃ¡genes relevantes.

### Flujo

```
1. Usuario ingresa query: "destinos paradisÃ­acos para luna de miel"
2. CLIP genera embedding del texto [512 dims]
3. MongoDB Vector Search retorna top 5 imÃ¡genes similares
4. Sistema construye contexto con metadatos de imÃ¡genes
5. Groq LLM genera respuesta usando el contexto
6. API retorna: { answer: "...", results: [...] }
```

### CÃ³digo de Prueba

```javascript
const response = await axios.post(`${API_BASE_URL}/query`, {
  query: "destinos paradisÃ­acos para luna de miel con playas de arena blanca",
  k: 5,
  includeAnswer: true,
});
```

### Por quÃ© es Multimodal

- **Entrada:** Texto natural
- **Procesamiento:** Embedding vectorial que entiende semÃ¡ntica
- **RecuperaciÃ³n:** ImÃ¡genes visualmente relacionadas (no solo keywords)
- **Salida:** Texto generado + imÃ¡genes

---

## ğŸ”§ Caso 2: Filtros HÃ­bridos (TEXTO + METADATOS â†’ TEXTO)

### DescripciÃ³n

Combina bÃºsqueda vectorial semÃ¡ntica con filtros estructurados tradicionales (categorÃ­as, tags).

### Flujo

```
1. Usuario: "hoteles de lujo con vista al mar" + category="hotel" + tags=["lujo"]
2. CLIP genera embedding del texto
3. MongoDB Vector Search + filtros estructurados
4. Solo retorna imÃ¡genes que cumplan AMBOS criterios:
   - Similitud vectorial alta
   - Coincidencia exacta de metadatos
5. LLM genera respuesta basada en resultados filtrados
```

### CÃ³digo de Prueba

```javascript
const response = await axios.post(`${API_BASE_URL}/query`, {
  query: "hoteles de lujo con vista al mar",
  category: "hotel",
  tags: ["lujo", "cinco-estrellas"],
  k: 5,
  includeAnswer: true,
});
```

### Por quÃ© es Multimodal

- Combina **bÃºsqueda semÃ¡ntica** (entiende "lujo" conceptualmente)
- Con **filtros tradicionales** (category = "hotel" exactamente)
- El resultado es mÃ¡s preciso que usar solo uno de los mÃ©todos

---

## ğŸ–¼ï¸ Caso 3: Similitud Visual (IMAGEN â†’ IMÃGENES)

### DescripciÃ³n

BÃºsqueda pura de similitud visual: dada una imagen, encuentra imÃ¡genes visualmente similares.

### Flujo

```
1. Usuario proporciona ID de imagen existente
2. Sistema recupera embedding de esa imagen (512 floats)
3. MongoDB Vector Search encuentra imÃ¡genes con embeddings similares
4. Retorna top K imÃ¡genes mÃ¡s parecidas visualmente
5. NO usa LLM (es bÃºsqueda vectorial pura)
```

### CÃ³digo de Prueba

```javascript
const response = await axios.get(
  `${API_BASE_URL}/similar/674e123abc456def789`,
  {
    params: { k: 5 },
  }
);
```

### Por quÃ© es Multimodal

- **Entrada:** Imagen (embedding visual)
- **Procesamiento:** ComparaciÃ³n de vectores en espacio CLIP
- **Salida:** ImÃ¡genes (no texto)
- Es **multimodal** porque usa embeddings que CLIP generÃ³ aprendiendo de pares texto-imagen

---

## ğŸ§  Caso 4: RAG Complejo (TEXTO â†’ IMÃGENES â†’ TEXTO enriquecido)

### DescripciÃ³n

Pipeline completo RAG: pregunta compleja que requiere sintetizar informaciÃ³n de mÃºltiples fuentes visuales.

### Flujo

```
1. Usuario hace pregunta compleja: "Â¿CuÃ¡les son las mejores opciones para un viaje romÃ¡ntico?"
2. CLIP genera embedding de la pregunta
3. Vector Search recupera TOP 10 imÃ¡genes relevantes (mÃ¡s contexto)
4. Sistema construye prompt detallado con:
   - TÃ­tulos de imÃ¡genes
   - CategorÃ­as
   - Descripciones (captions)
   - Tags
5. LLM analiza TODO el contexto y genera respuesta coherente
6. Retorna respuesta + evidencias (imÃ¡genes usadas)
```

### CÃ³digo de Prueba

```javascript
const response = await axios.post(`${API_BASE_URL}/query`, {
  query:
    "Â¿CuÃ¡les son las mejores opciones para un viaje romÃ¡ntico en pareja? Dame recomendaciones especÃ­ficas",
  k: 10, // MÃ¡s contexto para el LLM
  includeAnswer: true,
});
```

### Por quÃ© es Multimodal

- **Entrada:** Pregunta en lenguaje natural
- **RecuperaciÃ³n:** ImÃ¡genes + metadatos (multimodal)
- **SÃ­ntesis:** LLM integra informaciÃ³n visual y textual
- **Salida:** Respuesta que referencia contenido visual

---

## ğŸ”„ Caso 5: Cross-Modal (TEXTO â†’ IMÃGENES puras)

### DescripciÃ³n

BÃºsqueda cross-modal directa: texto busca imÃ¡genes visualmente relacionadas SIN generar respuesta de texto.

### Flujo

```
1. Usuario describe visualmente lo que busca: "paisajes montaÃ±osos con nieve"
2. CLIP genera embedding del TEXTO
3. MongoDB Vector Search compara ese embedding con embeddings de IMÃGENES
4. Retorna imÃ¡genes que "se parecen" visualmente a la descripciÃ³n
5. NO genera texto (solo retorna imÃ¡genes + scores)
```

### CÃ³digo de Prueba

```javascript
const response = await axios.get(`${API_BASE_URL}/search`, {
  params: {
    query: "paisajes montaÃ±osos con nieve y lagos cristalinos",
    k: 5,
  },
});
```

### Por quÃ© es Multimodal

- **La magia de CLIP:** El embedding de "paisajes montaÃ±osos" estÃ¡ en el **MISMO espacio vectorial** que el embedding de una foto real de montaÃ±as
- Texto e imagen son comparables directamente
- Es **cross-modal puro**: entrada texto, salida imÃ¡genes, sin keywords

---

## ğŸ”¬ Diferencias Clave

### Caso 1 vs Caso 5

| Aspecto        | Caso 1 (RAG)                | Caso 5 (Cross-modal)         |
| -------------- | --------------------------- | ---------------------------- |
| Endpoint       | POST /query                 | GET /search                  |
| LLM            | âœ… SÃ­ (genera respuesta)    | âŒ No                        |
| Salida         | Texto + ImÃ¡genes            | Solo ImÃ¡genes                |
| Use Case       | Responder preguntas         | Encontrar contenido visual   |
| Tiempo         | MÃ¡s lento (LLM aÃ±ade delay) | MÃ¡s rÃ¡pido (solo embedding)  |

### Caso 3 vs Caso 5

| Aspecto | Caso 3 (Imagenâ†’Imagen)       | Caso 5 (Textoâ†’Imagen)                |
| ------- | ---------------------------- | ------------------------------------ |
| Entrada | ID de imagen existente       | DescripciÃ³n textual                  |
| Proceso | Compara embeddings guardados | Genera embedding nuevo del texto     |
| Uso     | "MÃ¡s como esta"              | "Buscar imÃ¡genes de X"               |

---

## ğŸ“ˆ MÃ©tricas Relevantes por Tipo

### Para Casos con LLM (1, 2, 4)

- Tiempo de respuesta total
- Tiempo de vector search
- Tiempo de generaciÃ³n LLM
- Calidad de respuesta (subjetiva)
- Relevancia de contexto recuperado

### Para Casos sin LLM (3, 5)

- Tiempo de respuesta (solo embedding + vector search)
- PrecisiÃ³n de similitud (score cosine)
- Recall (Â¿encontrÃ³ lo esperado?)

---

## ğŸ“ Conceptos Clave

### Â¿QuÃ© hace "multimodal" a CLIP?

CLIP fue entrenado con **400 millones de pares (imagen, texto)** de internet. AprendiÃ³ a:

1. **Codificar imÃ¡genes** â†’ vector de 512 dimensiones
2. **Codificar textos** â†’ vector de 512 dimensiones (mismo espacio!)
3. **Maximizar similitud** entre pares correctos
4. **Minimizar similitud** entre pares incorrectos

**Resultado:** Un texto como "gato naranja" tiene un embedding MUY similar al embedding de una foto real de un gato naranja.

### Â¿Por quÃ© Vector Search?

BÃºsqueda tradicional (SQL LIKE):

```sql
SELECT * FROM images WHERE caption LIKE '%playa%'
```

- Solo encuentra coincidencias exactas de palabras
- "playa" â‰  "costa" â‰  "litoral" (aunque son lo mismo)

Vector Search:

```javascript
db.media.aggregate([
  {
    $vectorSearch: {
      queryVector: embedding_de("playa"),
      // Este embedding tambiÃ©n es similar a "costa", "arena", "mar"
    },
  },
]);
```

- Encuentra **conceptos similares** aunque usen palabras diferentes
- Entiende sinonimos, contexto, relaciones semÃ¡nticas

---

## ğŸš€ Resumen para tu Informe

**Nuestro sistema cubre TODOS los tipos de bÃºsqueda multimodal:**

1. âœ… **Texto â†’ Texto + ImÃ¡genes** (RAG clÃ¡sico)
2. âœ… **Texto + Filtros â†’ Texto** (HÃ­brido semÃ¡ntico/estructurado)
3. âœ… **Imagen â†’ ImÃ¡genes** (Similitud visual)
4. âœ… **Texto â†’ ImÃ¡genes â†’ Texto** (RAG complejo)
5. âœ… **Texto â†’ ImÃ¡genes** (Cross-modal puro)

**Esto satisface completamente el requisito del PDF:**

> "Casos de uso texto-texto, imagen-imagen, multimodal"

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… Crear Ã­ndice vectorial en MongoDB Atlas
2. âœ… Ejecutar `npm run test-cases`
3. âœ… Capturar screenshots de cada caso
4. âœ… Documentar mÃ©tricas reales en INFORME_FINAL.md
5. âœ… Explicar diferencias entre tipos de bÃºsqueda

---

**Autor:** Sistema RAG Multimodal - Universidad de Caldas 2025-2
