# ğŸ“‹ DocumentaciÃ³n de Casos de Prueba - Sistema RAG Multimodal

Este documento explica en detalle cÃ³mo funciona cada uno de los 5 casos de prueba del sistema RAG (Retrieval-Augmented Generation) con capacidades multimodales.

---

## ğŸ¯ VisiÃ³n General

El sistema implementa un pipeline RAG que combina:

- **CLIP** (openai/clip-vit-base-patch32): Modelo de embeddings multimodales de 512 dimensiones
- **MongoDB Atlas Vector Search**: BÃºsqueda de similitud coseno sobre embeddings
- **LLM (OpenAI GPT)**: GeneraciÃ³n de respuestas contextualizadas

---

## ğŸ“Š CASO 1: BÃºsqueda SemÃ¡ntica Simple (TEXTO â†’ TEXTO + IMÃGENES)

### ğŸ¯ Objetivo

Evaluar la capacidad del sistema para entender lenguaje natural y generar respuestas contextualizadas usando RAG completo.

### ğŸ”„ Flujo de EjecuciÃ³n

```
Usuario escribe query
    â†“
1. Generar embedding de texto con CLIP
    â†“
2. BÃºsqueda vectorial en MongoDB (similitud coseno)
    â†“
3. Recuperar top-5 imÃ¡genes mÃ¡s relevantes
    â†“
4. Enviar contexto + query al LLM
    â†“
5. LLM genera respuesta personalizada
    â†“
Retorna: ImÃ¡genes + Respuesta en lenguaje natural
```

### ğŸ“ Query de Ejemplo

```javascript
"destinos paradisÃ­acos para luna de miel con playas de arena blanca";
```

### ğŸ”§ ImplementaciÃ³n TÃ©cnica

**Request:**

```javascript
POST /api/rag/query
{
  query: "destinos paradisÃ­acos para luna de miel con playas de arena blanca",
  k: 5,
  includeAnswer: true
}
```

**Proceso Interno:**

1. **Embedding de texto**: CLIP convierte la query en vector de 512 dimensiones
2. **Vector Search**: MongoDB ejecuta bÃºsqueda con operador `$vectorSearch`
3. **Contexto**: Se construye un prompt con metadatos de las imÃ¡genes recuperadas
4. **LLM**: OpenAI GPT genera respuesta usando el contexto
5. **Respuesta**: JSON con imÃ¡genes + respuesta textual

### ğŸ“¤ Salida Esperada

- **ImÃ¡genes**: 5 destinos con playas tropicales (score > 0.85)
- **Respuesta LLM**: Texto descriptivo recomendando destinos especÃ­ficos
- **Tiempo**: ~300-500ms (bÃºsqueda) + ~1000-2000ms (LLM)

### ğŸ“ Conceptos Evaluados

- âœ… ComprensiÃ³n semÃ¡ntica de lenguaje natural
- âœ… BÃºsqueda vectorial precisa
- âœ… GeneraciÃ³n de texto contextualizado
- âœ… Pipeline RAG completo (Retrieve â†’ Augment â†’ Generate)

---

## ğŸ” CASO 2: Filtros HÃ­bridos (TEXTO + METADATOS â†’ TEXTO)

### ğŸ¯ Objetivo

Evaluar la combinaciÃ³n de bÃºsqueda vectorial semÃ¡ntica con filtros estructurados tradicionales.

### ğŸ”„ Flujo de EjecuciÃ³n

```
Usuario: query + filtro de categorÃ­a
    â†“
1. Aplicar filtro de categorÃ­a (MongoDB query tradicional)
    â†“
2. Generar embedding del texto
    â†“
3. BÃºsqueda vectorial SOLO en documentos filtrados
    â†“
4. Recuperar top-5 resultados
    â†“
5. LLM genera respuesta con contexto filtrado
    â†“
Retorna: Resultados hÃ­bridos (vectorial + filtros)
```

### ğŸ“ Query de Ejemplo

```javascript
"hoteles de lujo con vista al mar"
[Filtro: category=hotel]
```

### ğŸ”§ ImplementaciÃ³n TÃ©cnica

**Request:**

```javascript
POST /api/rag/query
{
  query: "hoteles de lujo con vista al mar",
  category: "hotel",  // Filtro estructurado
  k: 5,
  includeAnswer: true
}
```

**Proceso Interno:**

1. **Pre-filtro**: MongoDB aplica `{category: "hotel"}` antes de la bÃºsqueda vectorial
2. **Vector Search con filtro**:
   ```javascript
   {
     $vectorSearch: {
       index: "vector_search_index",
       path: "image_embedding",
       queryVector: [...512 dimensions...],
       filter: { category: { $eq: "hotel" } },  // â† Filtro integrado
       numCandidates: 50,
       limit: 5
     }
   }
   ```
3. **Contexto**: Solo hoteles que coinciden semÃ¡nticamente
4. **LLM**: Genera recomendaciones especÃ­ficas de hoteles

### ğŸ“¤ Salida Esperada

- **ImÃ¡genes**: Solo hoteles (category="hotel") que mencionan "lujo" o "vista al mar"
- **PrecisiÃ³n**: Mayor que bÃºsqueda pura (reduce falsos positivos)
- **Tiempo**: ~350-450ms (filtro + bÃºsqueda vectorial)

### ğŸ“ Conceptos Evaluados

- âœ… BÃºsqueda hÃ­brida (semÃ¡ntica + estructurada)
- âœ… Filtros de metadatos en vector search
- âœ… ReducciÃ³n de espacio de bÃºsqueda
- âœ… PrecisiÃ³n mejorada con contexto especÃ­fico

---

## ğŸ–¼ï¸ CASO 3: BÃºsqueda Multimodal (IMAGEN â†’ IMÃGENES)

### ğŸ¯ Objetivo

Evaluar la capacidad de CLIP para comparar similitud visual pura entre embeddings de imÃ¡genes.

### ğŸ”„ Flujo de EjecuciÃ³n

```
Sistema obtiene imagen de referencia de la BD
    â†“
1. Leer embedding de la imagen de referencia (ya existe en BD)
    â†“
2. BÃºsqueda vectorial usando ese embedding
    â†“
3. Calcular similitud coseno con todas las demÃ¡s imÃ¡genes
    â†“
4. Recuperar top-5 imÃ¡genes mÃ¡s similares visualmente
    â†“
Retorna: ImÃ¡genes similares (sin LLM, sin texto)
```

### ğŸ“ Proceso de Ejemplo

```javascript
Imagen de referencia: "Playa de San AndrÃ©s"
â†’ Busca: Otras playas con caracterÃ­sticas visuales similares
```

### ğŸ”§ ImplementaciÃ³n TÃ©cnica

**Request:**

```javascript
// 1. Obtener imagen de referencia
GET /api/rag/images?limit=1&category=destino

// 2. Buscar similares
GET /api/rag/similar/:mediaId?k=5
```

**Proceso Interno:**

1. **Leer embedding**: Recupera `image_embedding` del documento referencia
2. **Vector Search**:
   ```javascript
   {
     $vectorSearch: {
       queryVector: referenceImage.image_embedding,  // Vector de 512 dim
       path: "image_embedding",
       numCandidates: 50,
       limit: 5
     }
   }
   ```
3. **Similitud coseno**: MongoDB calcula distancia entre vectores
4. **Ranking**: Ordena por score descendente (1.0 = idÃ©ntico, 0.0 = diferente)

### ğŸ“¤ Salida Esperada

- **ImÃ¡genes**: Lugares con composiciÃ³n visual similar (colores, formas, escena)
- **Scores**: 0.75-0.95 (alta similitud visual)
- **Sin texto**: No usa LLM ni queries textuales
- **Tiempo**: ~250-350ms (solo bÃºsqueda vectorial)

### ğŸ“ Conceptos Evaluados

- âœ… Similitud visual pura (sin intervenciÃ³n de texto)
- âœ… Embeddings de imagen funcionando correctamente
- âœ… Capacidad de CLIP para capturar caracterÃ­sticas visuales
- âœ… Vector search sobre embeddings pre-calculados

### ğŸ§  Â¿Por quÃ© funciona?

CLIP fue entrenado con 400M de pares imagen-texto, aprendiendo a:

- Identificar objetos, escenas y composiciones
- Reconocer colores, texturas y patrones
- Entender contexto visual (playa, montaÃ±a, ciudad, etc.)

---

## ğŸ’¬ CASO 4: RAG Complejo con LLM (TEXTO â†’ IMÃGENES â†’ TEXTO enriquecido)

### ğŸ¯ Objetivo

Evaluar la integraciÃ³n completa del pipeline RAG con queries complejas que requieren razonamiento y sÃ­ntesis.

### ğŸ”„ Flujo de EjecuciÃ³n

```
Usuario: Query compleja con mÃºltiples intenciones
    â†“
1. Generar embedding del texto (CLIP)
    â†“
2. BÃºsqueda vectorial amplia (k=10 para mÃ¡s contexto)
    â†“
3. Recuperar imÃ¡genes de mÃºltiples categorÃ­as
    â†“
4. Construir contexto rico con metadatos diversos
    â†“
5. LLM analiza y sintetiza informaciÃ³n
    â†“
6. Genera respuesta estructurada con recomendaciones
    â†“
Retorna: Respuesta compleja + imÃ¡genes de soporte
```

### ğŸ“ Query de Ejemplo

```javascript
"Â¿CuÃ¡les son las mejores opciones para un viaje romÃ¡ntico en pareja?
Dame recomendaciones especÃ­ficas de destinos, hoteles y actividades"
```

### ğŸ”§ ImplementaciÃ³n TÃ©cnica

**Request:**

```javascript
POST /api/rag/query
{
  query: "Â¿CuÃ¡les son las mejores opciones para un viaje romÃ¡ntico en pareja?...",
  k: 10,  // â† MÃ¡s resultados para contexto rico
  includeAnswer: true
}
```

**Proceso Interno:**

1. **Embedding multimodal**: CLIP entiende "romÃ¡ntico", "pareja", "viaje"
2. **BÃºsqueda amplia**: Recupera 10 documentos (destinos, hoteles, actividades)
3. **Contexto estructurado**:
   ```
   Contexto para el LLM:
   - Imagen 1: Hotel Casa Colonial (hotel, romÃ¡ntico, vista-mar)
   - Imagen 2: Playa de Varadero (destino, playa, tropical)
   - Imagen 3: Cena romÃ¡ntica (gastronomÃ­a, pareja)
   - ... (7 mÃ¡s)
   ```
4. **Prompt al LLM**:

   ```
   BasÃ¡ndote en las siguientes opciones turÃ­sticas:
   [contexto de 10 imÃ¡genes]

   Responde: Â¿CuÃ¡les son las mejores opciones para un viaje romÃ¡ntico?
   ```

5. **LLM genera**: Respuesta estructurada con mÃºltiples recomendaciones

### ğŸ“¤ Salida Esperada

- **ImÃ¡genes**: 10 resultados variados (hoteles, destinos, actividades)
- **Respuesta LLM**: Texto largo (~200-400 palabras) con:
  - Recomendaciones de destinos especÃ­ficos
  - Hoteles sugeridos con caracterÃ­sticas
  - Actividades romÃ¡nticas
  - JustificaciÃ³n de cada recomendaciÃ³n
- **Tiempo**: ~400-600ms (bÃºsqueda) + ~2000-4000ms (LLM genera mÃ¡s texto)

### ğŸ“ Conceptos Evaluados

- âœ… Queries complejas multi-intenciÃ³n
- âœ… RecuperaciÃ³n de contexto diverso
- âœ… SÃ­ntesis de informaciÃ³n por LLM
- âœ… GeneraciÃ³n de respuestas estructuradas
- âœ… RAG completo con razonamiento

### ğŸ”¬ Diferencia con Caso 1

| Aspecto           | Caso 1                | Caso 4                               |
| ----------------- | --------------------- | ------------------------------------ |
| **Query**         | Simple y directa      | Compleja, multi-intenciÃ³n            |
| **k**             | 5 resultados          | 10 resultados                        |
| **Contexto**      | Focalizado            | Diverso y rico                       |
| **Respuesta LLM** | Corta (~100 palabras) | Larga y estructurada (~300 palabras) |
| **PropÃ³sito**     | BÃºsqueda especÃ­fica   | AnÃ¡lisis y recomendaciones           |

---

## ğŸŒ CASO 5: BÃºsqueda Multimodal Cross-Modal (TEXTO â†’ IMÃGENES sin LLM)

### ğŸ¯ Objetivo

Evaluar la capacidad de CLIP para mapear texto e imagen en el mismo espacio vectorial, permitiendo bÃºsqueda directa textoâ†’imagen.

### ğŸ”„ Flujo de EjecuciÃ³n

```
Usuario: DescripciÃ³n textual de imagen deseada
    â†“
1. Generar embedding de TEXTO con CLIP
    â†“
2. BÃºsqueda vectorial directa (texto busca imÃ¡genes)
    â†“
3. Similitud coseno: vector_texto vs vector_imagen
    â†“
4. Recuperar imÃ¡genes visualmente similares al concepto
    â†“
Retorna: ImÃ¡genes (SIN pasar por LLM)
```

### ğŸ“ Query de Ejemplo

```javascript
"paisajes montaÃ±osos con nieve y lagos cristalinos";
```

### ğŸ”§ ImplementaciÃ³n TÃ©cnica

**Request:**

```javascript
GET /api/rag/search?query=paisajes montaÃ±osos con nieve...&k=5
```

**Proceso Interno:**

1. **Embedding de texto**: CLIP transforma texto en vector de 512 dimensiones
2. **ComparaciÃ³n directa**:

   ```javascript
   embedding_texto = CLIP.encode_text("paisajes montaÃ±osos...")
   // Vector resultado: [0.23, -0.45, 0.67, ..., 0.12]  (512 dims)

   // MongoDB busca imÃ¡genes con embeddings similares
   $vectorSearch: {
     queryVector: embedding_texto,  // â† Vector de TEXTO
     path: "image_embedding",        // â† Busca en embeddings de IMAGEN
     limit: 5
   }
   ```

3. **Similitud cross-modal**: Calcula coseno entre:
   - Vector de TEXTO ("paisajes montaÃ±osos...")
   - Vectores de IMAGEN (fotos reales en la BD)
4. **NO usa LLM**: Devuelve imÃ¡genes directamente

### ğŸ“¤ Salida Esperada

- **ImÃ¡genes**: Fotos de montaÃ±as con nieve y lagos (score 0.70-0.85)
- **Sin respuesta textual**: Solo metadata de las imÃ¡genes
- **Tiempo**: ~200-300ms (muy rÃ¡pido, sin LLM)

### ğŸ“ Conceptos Evaluados

- âœ… **Cross-modal search**: Texto busca imÃ¡genes directamente
- âœ… **Espacio vectorial compartido**: CLIP aprende texto e imagen en mismo espacio
- âœ… **BÃºsqueda semÃ¡ntica visual**: "nieve" encuentra imÃ¡genes nevadas
- âœ… **Eficiencia**: Sin overhead de LLM

### ğŸ§  Â¿Por quÃ© funciona el cross-modal search?

**Entrenamiento de CLIP:**

```
Imagen de montaÃ±a nevada  â”€â”€â†’  CLIP Encoder  â”€â”€â†’  [0.2, -0.4, 0.6, ...]
                                                         â†“
                                                    (512 dims)
                                                         â†‘
Texto "montaÃ±a nevada"    â”€â”€â†’  CLIP Encoder  â”€â”€â†’  [0.2, -0.4, 0.6, ...]
```

CLIP fue entrenado con **contrastive learning** para que:

- Imagen de "perro" y texto "perro" â†’ Vectores CERCANOS (similitud alta)
- Imagen de "perro" y texto "gato" â†’ Vectores LEJANOS (similitud baja)

**Resultado**: Podemos usar vector de texto para buscar imÃ¡genes que "se vean" como ese texto.

### ğŸ”¬ Diferencia con Caso 1

| Aspecto       | Caso 1 (RAG)             | Caso 5 (Cross-modal)    |
| ------------- | ------------------------ | ----------------------- |
| **Input**     | Texto                    | Texto                   |
| **Output**    | ImÃ¡genes + Texto LLM     | Solo imÃ¡genes           |
| **Usa LLM**   | âœ… SÃ­                    | âŒ No                   |
| **Tiempo**    | ~1500ms                  | ~250ms                  |
| **PropÃ³sito** | Respuesta conversacional | BÃºsqueda visual directa |
| **Endpoint**  | `POST /query`            | `GET /search`           |

---

## ğŸ“Š ComparaciÃ³n de Casos de Prueba

| Caso  | Tipo              | Input           | Output                 | Usa LLM | Tiempo  | Complejidad |
| ----- | ----------------- | --------------- | ---------------------- | ------- | ------- | ----------- |
| **1** | RAG SemÃ¡ntico     | Texto           | ImÃ¡genes + Texto       | âœ…      | ~1500ms | Media       |
| **2** | RAG HÃ­brido       | Texto + Filtros | ImÃ¡genes + Texto       | âœ…      | ~1200ms | Media       |
| **3** | Visual Similarity | Imagen          | ImÃ¡genes               | âŒ      | ~300ms  | Baja        |
| **4** | RAG Complejo      | Texto complejo  | ImÃ¡genes + Texto largo | âœ…      | ~3000ms | Alta        |
| **5** | Cross-Modal       | Texto           | Solo imÃ¡genes          | âŒ      | ~250ms  | Baja        |

---

## ğŸ”§ Arquitectura TÃ©cnica Completa

### Stack TecnolÃ³gico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND / API CLIENT                 â”‚
â”‚                  (axios, test-cases.js)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NODE.JS API SERVER                     â”‚
â”‚                  (Express + Controllers)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ POST /api/rag/query     â†’ RAG completo (LLM)         â”‚
â”‚  â€¢ GET  /api/rag/search    â†’ Solo bÃºsqueda vectorial    â”‚
â”‚  â€¢ GET  /api/rag/similar   â†’ Similitud imagen-imagen    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                           â†“                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   CLIP   â”‚            â”‚  MongoDB Atlas  â”‚   â”‚ OpenAI   â”‚
    â”‚  Server  â”‚            â”‚  Vector Search  â”‚   â”‚   API    â”‚
    â”‚ (Python) â”‚            â”‚                 â”‚   â”‚  (GPT)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                           â†“                  â†“
    Embeddings              Vector Index          GeneraciÃ³n
    512 dims                (cosine sim)          de texto
```

### Flujo de Datos - RAG Completo (Caso 1, 2, 4)

```
1. USER â†’ "destinos paradisÃ­acos"
         â†“
2. API â†’ CLIP Server: encode_text("destinos paradisÃ­acos")
         â†“
3. CLIP â†’ API: [0.23, -0.45, 0.67, ..., 0.12] (512 dims)
         â†“
4. API â†’ MongoDB: $vectorSearch con queryVector
         â†“
5. MongoDB â†’ API: Top-5 documentos con scores
         â†“
6. API construye contexto:
   "Basado en estas imÃ¡genes:
    - Playa Varadero (playa, tropical, arena-blanca)
    - Maldivas Resort (hotel, lujo, playa)
    ..."
         â†“
7. API â†’ OpenAI: prompt + contexto
         â†“
8. OpenAI â†’ API: "Te recomiendo visitar Varadero porque..."
         â†“
9. API â†’ USER: {results: [...], answer: "..."}
```

### MongoDB Vector Search - ConfiguraciÃ³n

**Ãndice vectorial requerido:**

```json
{
  "fields": [
    {
      "type": "vector",
      "path": "image_embedding",
      "numDimensions": 512,
      "similarity": "cosine"
    },
    {
      "type": "filter",
      "path": "category"
    },
    {
      "type": "filter",
      "path": "tags"
    }
  ]
}
```

**Query de ejemplo:**

```javascript
db.media.aggregate([
  {
    $vectorSearch: {
      index: "vector_search_index",
      path: "image_embedding",
      queryVector: [0.23, -0.45, ..., 0.12],  // 512 nÃºmeros
      numCandidates: 50,  // Pre-filtro
      limit: 5,           // Resultados finales
      filter: { category: { $eq: "hotel" } }  // Opcional
    }
  },
  {
    $project: {
      title: 1,
      category: 1,
      tags: 1,
      caption: 1,
      score: { $meta: "vectorSearchScore" }
    }
  }
])
```

---

## ğŸ“ˆ MÃ©tricas de Rendimiento

### Tiempos Esperados por Caso

| Componente       | Caso 1      | Caso 2      | Caso 3     | Caso 4      | Caso 5     |
| ---------------- | ----------- | ----------- | ---------- | ----------- | ---------- |
| Embedding (CLIP) | 50ms        | 50ms        | 0ms\*      | 50ms        | 50ms       |
| Vector Search    | 200ms       | 250ms       | 200ms      | 300ms       | 200ms      |
| LLM Generation   | 1000ms      | 800ms       | 0ms        | 2500ms      | 0ms        |
| **TOTAL**        | **~1250ms** | **~1100ms** | **~200ms** | **~2850ms** | **~250ms** |

\* Caso 3 usa embedding pre-calculado de imagen existente

### Factores que Afectan el Rendimiento

1. **CLIP Embedding**:

   - Depende de GPU disponible
   - ~20-50ms en CPU
   - ~5-10ms en GPU

2. **MongoDB Vector Search**:

   - Depende de `numCandidates` (mÃ¡s candidatos = mÃ¡s lento)
   - Ãndice vectorial DEBE estar creado
   - Red (si Atlas estÃ¡ en cloud)

3. **LLM (OpenAI)**:
   - Depende de longitud de respuesta
   - Respuestas cortas: ~800-1200ms
   - Respuestas largas: ~2000-4000ms
   - Puede variar segÃºn carga de OpenAI

---

## ğŸ¯ Casos de Uso Real

### Caso 1 - BÃºsqueda Conversacional

**Escenario**: Chatbot de agencia de viajes

```
Usuario: "Quiero ir a un lugar cÃ¡lido con playa"
Sistema: Busca semÃ¡nticamente â†’ Retorna playas tropicales + recomendaciones
```

### Caso 2 - BÃºsqueda Filtrada

**Escenario**: Filtro de categorÃ­a en sitio web

```
Usuario: Selecciona "Solo Hoteles" + busca "resort con piscina"
Sistema: Filtra por category=hotel â†’ Busca vectorialmente "resort con piscina"
```

### Caso 3 - BÃºsqueda por Similitud Visual

**Escenario**: "MÃ¡s como este"

```
Usuario: Click en imagen de playa caribeÃ±a
Sistema: Encuentra visualmente â†’ Retorna playas con colores/composiciÃ³n similar
```

### Caso 4 - Asistente Inteligente

**Escenario**: PlanificaciÃ³n compleja de viaje

```
Usuario: "Â¿QuÃ© hacer en una luna de miel de 7 dÃ­as?"
Sistema: Busca ampliamente â†’ LLM sintetiza itinerario completo
```

### Caso 5 - BÃºsqueda Visual RÃ¡pida

**Escenario**: GalerÃ­a de imÃ¡genes

```
Usuario: Escribe "atardecer en la playa"
Sistema: Retorna fotos de atardeceres â†’ Sin texto adicional
```

---

## ğŸš€ EjecuciÃ³n de los Tests

### Prerequisitos

```bash
# 1. MongoDB Atlas con Ã­ndice vectorial creado
# 2. Servidor Node.js corriendo
npm start

# 3. Servicio Python CLIP activo
python python/clip_service.py

# 4. ImÃ¡genes de muestra cargadas
npm run load-samples
```

### Ejecutar Todos los Casos

```bash
npm run test-cases
```

### Ejecutar Caso Individual

```javascript
const { testCase1 } = require("./scripts/test-cases");
await testCase1();
```

---

## ğŸ“Š InterpretaciÃ³n de Resultados

### Scores de Similitud

- **0.90 - 1.00**: Coincidencia casi perfecta (misma imagen o muy similar)
- **0.75 - 0.90**: Alta similitud (tema/escena muy relacionado)
- **0.60 - 0.75**: Similitud moderada (concepto relacionado)
- **0.40 - 0.60**: Baja similitud (conexiÃ³n dÃ©bil)
- **< 0.40**: No relacionado

### Calidad de Respuestas LLM

- âœ… **Buena**: Menciona imÃ¡genes especÃ­ficas del contexto
- âœ… **Buena**: Proporciona detalles precisos (nombres, ubicaciones)
- âš ï¸ **Regular**: Respuesta genÃ©rica sin usar contexto
- âŒ **Mala**: InformaciÃ³n incorrecta o alucinaciones

---

## ğŸ” Troubleshooting

### Caso 2 devuelve 0 resultados

**Causa**: Filtro muy restrictivo (ej: tags inexistentes)
**SoluciÃ³n**: Usar solo `category` sin tags especÃ­ficos

### Caso 4 muestra `[object Object]ms`

**Causa**: Orden incorrecto de parÃ¡metros en `displayResults()`
**SoluciÃ³n**: Verificar que sea `(testNumber, testName, query, results, responseTime)`

### Todos los casos fallan

**Causa**: Ãndice vectorial no creado en MongoDB Atlas
**SoluciÃ³n**: Crear Ã­ndice `vector_search_index` en colecciÃ³n `media`

---

## ğŸ“š Referencias

- **CLIP Paper**: https://arxiv.org/abs/2103.00020
- **MongoDB Vector Search**: https://www.mongodb.com/docs/atlas/atlas-vector-search/
- **RAG Pattern**: https://arxiv.org/abs/2005.11401
- **OpenAI API**: https://platform.openai.com/docs

---

**Ãšltima actualizaciÃ³n**: Diciembre 2025
